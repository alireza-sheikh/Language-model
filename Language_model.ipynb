{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SNIZB7quOnG9",
        "y12gki3jOv2Q",
        "FVpxb57G8s2c",
        "LDR3oWdUO4Sa",
        "uQYzpJOObJEI",
        "cjJAPD4lnrUd",
        "JA0ZHXnUt8lj",
        "l3c-qLTpw1Pw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Download and read the data**"
      ],
      "metadata": {
        "id": "SNIZB7quOnG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we download the data and take a walkthrough it."
      ],
      "metadata": {
        "id": "AqeSD222bs4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "ctlZ_m7VbctO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "dd22155b-3f6b-4df5-bfce-7177c364daf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 13:53:18--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-02-22 13:53:18 (26.5 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Na8JEA9DZkww"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "0e514d36-cf1f-4ed9-a6f2-d650c6d3bfb4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "1351e2e1-72f8-4388-d431-16caa56a0fbc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "79beaeca-b947-47cb-a452-4d6a8ce6c777"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the input text. Convert raw text as a string to sequences of integers.  \n",
        "In this tokenizer, convert the characters to integers, so this is character level tokenizer.  \n",
        "**encode** takes string and convert it to integers and **decode** takes integers and convert them to string. "
      ],
      "metadata": {
        "id": "Cg9MpRVynQUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "f1740e3d-62a3-4624-d854-cfeb2ed03988"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch \n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "bb36f757-27e5-4c00-d92e-428c6e1d202d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of feed all the dataset into model once, we should feed the network with chunks of data because of it is very expensive computationally. "
      ],
      "metadata": {
        "id": "qdZzBgvKqpiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detremine size of each chunk as block_size\n",
        "block_size = 8   \n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "2b4eb6bc-01ce-4c23-cc52-29f295a0e73d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input of transformer\n",
        "x = train_data[:block_size]\n",
        "# Next block size of charactres, offest by one\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "9c3dfdff-86d4-493d-e6b7-04d98338da39"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    # Input of model\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    # Ground truth of model output\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "76f08ff0-4e2e-4fbb-c9cf-2d08b11e0b0f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "f35938f8-dd8b-48f4-ce9b-2fe14c150dc6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Simplest model: BigramLanguageModel**"
      ],
      "metadata": {
        "id": "y12gki3jOv2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we implement a simple model and train it on the dataset"
      ],
      "metadata": {
        "id": "Slmyxh1CcL5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C) = (4, 8, 64)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    # This function all the chunks\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "f5cc6605-4338-4092-8053-c50596d78bba"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
            "wnYWmnxKWWev-tDqXErVKLgJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "batch_size = 32\n",
        "for steps in range(10000):  \n",
        "    \n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (steps+1) % 500 == 0:\n",
        "      print(f\"loss in step {steps+1} is {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "c3e1120f-6fbb-443a-8df7-90f996a3bf8b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in step 500 is 4.1035356521606445\n",
            "loss in step 1000 is 3.704137086868286\n",
            "loss in step 1500 is 3.4056379795074463\n",
            "loss in step 2000 is 3.106309175491333\n",
            "loss in step 2500 is 3.0473382472991943\n",
            "loss in step 3000 is 2.8964755535125732\n",
            "loss in step 3500 is 2.6635422706604004\n",
            "loss in step 4000 is 2.5542068481445312\n",
            "loss in step 4500 is 2.620234727859497\n",
            "loss in step 5000 is 2.536130905151367\n",
            "loss in step 5500 is 2.475377082824707\n",
            "loss in step 6000 is 2.6574454307556152\n",
            "loss in step 6500 is 2.460270881652832\n",
            "loss in step 7000 is 2.4518814086914062\n",
            "loss in step 7500 is 2.373178482055664\n",
            "loss in step 8000 is 2.425344228744507\n",
            "loss in step 8500 is 2.3889195919036865\n",
            "loss in step 9000 is 2.2895452976226807\n",
            "loss in step 9500 is 2.4984893798828125\n",
            "loss in step 10000 is 2.382369041442871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "0cfd65d8-bc8e-4e40-b6ea-db156cf4bba1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
            "RIfans picspeserer hee tha,\n",
            "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
            "ELIS me hurf lal y, ma dus pe athouo\n",
            "BEY:! Indy; by s afreanoo adicererupa anse tecorro llaus a!\n",
            "OLeneerithesinthengove fal amas trr\n",
            "TI ar I t, mes, n IUSt my w, fredeeyove\n",
            "THek' merer, dd\n",
            "We ntem lud engitheso; cer ize helorowaginte the?\n",
            "Thak orblyoruldvicee chot, p,\n",
            "Bealivolde Th li\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see model prediction is so weird and it is because we used the simplest model.\n",
        "Now lets build better models."
      ],
      "metadata": {
        "id": "ndwY7zmFx_jJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Set hyperparameters and create useful function for rest of notebook**"
      ],
      "metadata": {
        "id": "FVpxb57G8s2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "id": "4Hrg5pTXdH86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "PvaJBS-Rc7_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "G0w21EdXc89K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Add single head self attention**\n",
        "In the last model, characters were predicted only based on last character, now by adding 'Head' module, nodes can communicate to the past nodes and we can expect this will improve result."
      ],
      "metadata": {
        "id": "LDR3oWdUO4Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This module implements a single head of self attention\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))        \n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "9-groTh4S5io"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Single_Haed_Attention_BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_head = Head(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_embd = self.token_embedding_table(idx) # (B, T, C=n_emd)\n",
        "        pos_embd = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_embd + pos_embd  #(B, T, C)\n",
        "        x = self.sa_head(x)\n",
        "        logits = self.lm_head(x)  # (B, T, C=vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    # This function generates all the chunks\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "iGuZfnbeOgEx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = Single_Haed_Attention_BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySeJJnieW7C3",
        "outputId": "cafd733f-8cc9-402d-ade5-6fea889f4873"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.022721 M parameters\n",
            "step 0: train loss 4.1554, val loss 4.1559\n",
            "step 100: train loss 3.0067, val loss 3.0223\n",
            "step 200: train loss 2.8535, val loss 2.8669\n",
            "step 300: train loss 2.7685, val loss 2.7696\n",
            "step 400: train loss 2.7110, val loss 2.7007\n",
            "step 500: train loss 2.6484, val loss 2.6545\n",
            "step 600: train loss 2.5910, val loss 2.5880\n",
            "step 700: train loss 2.5490, val loss 2.5595\n",
            "step 800: train loss 2.5110, val loss 2.5168\n",
            "step 900: train loss 2.4869, val loss 2.4981\n",
            "step 1000: train loss 2.4654, val loss 2.4828\n",
            "step 1100: train loss 2.4565, val loss 2.4760\n",
            "step 1200: train loss 2.4462, val loss 2.4489\n",
            "step 1300: train loss 2.4328, val loss 2.4445\n",
            "step 1400: train loss 2.4172, val loss 2.4268\n",
            "step 1500: train loss 2.4098, val loss 2.4340\n",
            "step 1600: train loss 2.4104, val loss 2.4211\n",
            "step 1700: train loss 2.4070, val loss 2.4252\n",
            "step 1800: train loss 2.4016, val loss 2.4143\n",
            "step 1900: train loss 2.3887, val loss 2.4102\n",
            "step 2000: train loss 2.3871, val loss 2.4030\n",
            "step 2100: train loss 2.3836, val loss 2.4016\n",
            "step 2200: train loss 2.3815, val loss 2.4079\n",
            "step 2300: train loss 2.3816, val loss 2.4013\n",
            "step 2400: train loss 2.3722, val loss 2.4047\n",
            "step 2500: train loss 2.3692, val loss 2.4064\n",
            "step 2600: train loss 2.3775, val loss 2.4005\n",
            "step 2700: train loss 2.3646, val loss 2.3965\n",
            "step 2800: train loss 2.3671, val loss 2.3906\n",
            "step 2900: train loss 2.3709, val loss 2.3915\n",
            "step 3000: train loss 2.3692, val loss 2.3942\n",
            "step 3100: train loss 2.3604, val loss 2.3834\n",
            "step 3200: train loss 2.3600, val loss 2.3857\n",
            "step 3300: train loss 2.3505, val loss 2.3846\n",
            "step 3400: train loss 2.3478, val loss 2.3840\n",
            "step 3500: train loss 2.3480, val loss 2.3771\n",
            "step 3600: train loss 2.3475, val loss 2.3788\n",
            "step 3700: train loss 2.3450, val loss 2.3694\n",
            "step 3800: train loss 2.3404, val loss 2.3822\n",
            "step 3900: train loss 2.3479, val loss 2.3762\n",
            "step 4000: train loss 2.3350, val loss 2.3777\n",
            "step 4100: train loss 2.3315, val loss 2.3686\n",
            "step 4200: train loss 2.3357, val loss 2.3623\n",
            "step 4300: train loss 2.3355, val loss 2.3758\n",
            "step 4400: train loss 2.3383, val loss 2.3702\n",
            "step 4500: train loss 2.3259, val loss 2.3702\n",
            "step 4600: train loss 2.3304, val loss 2.3655\n",
            "step 4700: train loss 2.3332, val loss 2.3725\n",
            "step 4800: train loss 2.3323, val loss 2.3687\n",
            "step 4900: train loss 2.3306, val loss 2.3566\n",
            "step 4999: train loss 2.3219, val loss 2.3611\n",
            "\n",
            "BUEKINARD NO LI:\n",
            "And ilvele yo fely tinolir.\n",
            "\n",
            "SGI monggo m fasst yo deaubis the ereerd\n",
            "Bar cther dour isenookieve I't O:\n",
            "lT:\n",
            "I lo piceetre?\n",
            "Teman te\n",
            "Sorme fous osis is ithis ud\n",
            "Thieno in bead ym owthand is cowre pleance llomep hat: core,\n",
            "Loundo,\n",
            "PArol\n",
            "PEds picead!\n",
            "Meng llima unes gs youreas ds!\n",
            "EBO:\n",
            "TIO:\n",
            "herawhe ainon mapr, ubouril hadd ne yold slime,\n",
            "Me cks mar my pas, the se. We bous.\n",
            "\n",
            "DUSom pan.\n",
            "\n",
            "O fend I aino shof marovincig ht fo pawom thait fer, Gorr hor de slle fofud\n",
            "As mowlo quurarmse lemy he felindwo.\n",
            "YA:\n",
            "TAs Vougendit he theigtaigcarvaimelor Adang.\n",
            "\n",
            "Thomars es whe, nche are winod;\n",
            "to harg'd the shato thot ongre thandr fror me st seer:\n",
            "Nome otesl pl fis beemp dty me we feirg oneclof tha ncut;\n",
            "D I INCII yr,\n",
            "Tow avedy sirone kered wines cous, thernd bickit sens,\n",
            "AL:\n",
            "So othof tesr wato h'dend ber alegnay im aikn blaithe bout tish gun inde no\n",
            "Whe prom bllaverat.\n",
            "Yoouy worimad wind to ost sadre;\n",
            "PUThorthe thad by crinos yu thaty hor by I mis have htisr fo,\n",
            "hionges I: weay wono isptsh owrice beem hiefr dithagwelim the'set,\n",
            "I kquingans honos inco in ksig ars thourls:\n",
            "K:\n",
            "Le shan atheeare fre ardut vilerd,\n",
            "HAnd st kn?\n",
            "NGLI:\n",
            "I pay iet try haichage hamy odis\n",
            "\n",
            "Fes:\n",
            "Am allllin nod bes ilon alfikes sloull yo, wispee nerd?\n",
            "\n",
            "Mavergie hiner cachany hu a pad thing arseanware theat is ts lence pe st lorow homeervex ithe ses.\n",
            "\n",
            "Do frith pterat sor'liextl, on a whay toer pre cintar Pagchyoou ow cene odod, vanctenen thanck anatho' apilplord the ot cagavexpo tonis athenl owon sheid!\n",
            "\n",
            "HAMy dly nbelo ndrealvisante fer ors is bous: hreavead avend sith ceslag.\n",
            "\n",
            "DLof I or sar sthe me,\n",
            "Sou ane borareacthivel my nevet gn chy lt it berat arddir ceiclout knes Handurpel wany rvoary!\n",
            "\n",
            "Thitis.\n",
            "\n",
            "SA:\n",
            "Cady ny minour'th ow marasu ires un the dlasst thor im, ak Dis ss tre te been st thes, G\n",
            "WAd; jeoun we thiqutitisins naf!\n",
            "Agonce!\n",
            "Hers hiten tows.\n",
            "\n",
            "KI orry st?\n",
            "\n",
            "Th tun od on cigtenamet buthe,\n",
            "My his ede thilloudis onche sarwin yoset: hor iven otengawicik me wr,\n",
            "Wofes st gonme at dowim ins inced;\n",
            "Th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note on single head attention model:\n",
        "As we expected adding sigle head attention improve loss from 2.38 to 2.36"
      ],
      "metadata": {
        "id": "umojA6sqakW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Add multi head attention to model**"
      ],
      "metadata": {
        "id": "uQYzpJOObJEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        # self.proj = nn.Linear(n_embd, n_embd)\n",
        "        # self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # out = self.proj(out)\n",
        "        # out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "g8bUw6sGdhUl"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Multi_Haed_Attention_BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_head = MultiHeadAttention(4, n_embd//4)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_embd = self.token_embedding_table(idx) # (B, T, C=n_emd)\n",
        "        pos_embd = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_embd + pos_embd  #(B, T, C)\n",
        "        x = self.sa_head(x)\n",
        "        logits = self.lm_head(x)  # (B, T, C=vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    # This function generates all the chunks\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "ycYm_ZUHd5Lj"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "xdOXoxRulu0c"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Multi_Haed_Attention_BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZM8Y4OwePYY",
        "outputId": "1a6ca777-7667-4694-ce8c-afd82c1f1e89"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.022721 M parameters\n",
            "step 0: train loss 4.1527, val loss 4.1546\n",
            "step 100: train loss 2.9981, val loss 3.0320\n",
            "step 200: train loss 2.7522, val loss 2.7731\n",
            "step 300: train loss 2.6735, val loss 2.6675\n",
            "step 400: train loss 2.6165, val loss 2.6202\n",
            "step 500: train loss 2.5822, val loss 2.5803\n",
            "step 600: train loss 2.5551, val loss 2.5532\n",
            "step 700: train loss 2.5279, val loss 2.5284\n",
            "step 800: train loss 2.4998, val loss 2.5009\n",
            "step 900: train loss 2.4681, val loss 2.4806\n",
            "step 1000: train loss 2.4438, val loss 2.4546\n",
            "step 1100: train loss 2.4248, val loss 2.4343\n",
            "step 1200: train loss 2.4049, val loss 2.4182\n",
            "step 1300: train loss 2.3971, val loss 2.3991\n",
            "step 1400: train loss 2.3843, val loss 2.3876\n",
            "step 1500: train loss 2.3539, val loss 2.3705\n",
            "step 1600: train loss 2.3532, val loss 2.3602\n",
            "step 1700: train loss 2.3336, val loss 2.3477\n",
            "step 1800: train loss 2.3263, val loss 2.3361\n",
            "step 1900: train loss 2.3083, val loss 2.3292\n",
            "step 2000: train loss 2.3036, val loss 2.3210\n",
            "step 2100: train loss 2.2937, val loss 2.3154\n",
            "step 2200: train loss 2.2819, val loss 2.2993\n",
            "step 2300: train loss 2.2751, val loss 2.2932\n",
            "step 2400: train loss 2.2574, val loss 2.2880\n",
            "step 2500: train loss 2.2526, val loss 2.2755\n",
            "step 2600: train loss 2.2480, val loss 2.2744\n",
            "step 2700: train loss 2.2324, val loss 2.2606\n",
            "step 2800: train loss 2.2301, val loss 2.2539\n",
            "step 2900: train loss 2.2200, val loss 2.2549\n",
            "step 3000: train loss 2.2071, val loss 2.2547\n",
            "step 3100: train loss 2.2110, val loss 2.2369\n",
            "step 3200: train loss 2.1931, val loss 2.2353\n",
            "step 3300: train loss 2.1854, val loss 2.2309\n",
            "step 3400: train loss 2.1860, val loss 2.2301\n",
            "step 3500: train loss 2.1841, val loss 2.2163\n",
            "step 3600: train loss 2.1705, val loss 2.2135\n",
            "step 3700: train loss 2.1678, val loss 2.2117\n",
            "step 3800: train loss 2.1663, val loss 2.2022\n",
            "step 3900: train loss 2.1576, val loss 2.1940\n",
            "step 4000: train loss 2.1633, val loss 2.2106\n",
            "step 4100: train loss 2.1541, val loss 2.1944\n",
            "step 4200: train loss 2.1503, val loss 2.2048\n",
            "step 4300: train loss 2.1538, val loss 2.1854\n",
            "step 4400: train loss 2.1460, val loss 2.1995\n",
            "step 4500: train loss 2.1475, val loss 2.1963\n",
            "step 4600: train loss 2.1274, val loss 2.1852\n",
            "step 4700: train loss 2.1326, val loss 2.1796\n",
            "step 4800: train loss 2.1334, val loss 2.1824\n",
            "step 4900: train loss 2.1161, val loss 2.1724\n",
            "step 4999: train loss 2.1256, val loss 2.1860\n",
            "\n",
            "Sy crpuock; for I goan fan may counf't bese and: tliny, kllekeelf best wovay: s\n",
            "HoAN EN Yaize soescunh Romatime, solt\n",
            "hall sorets If me ce.\n",
            "\n",
            "DrAR Seche I lordy all yeld mones:\n",
            "And I shate itt you thar cothe\n",
            "Whe hey Pounkessilem.\n",
            "\n",
            "QINAr:\n",
            "The werl cheis o acr coaw gamy giet to baiind nousmy beve\n",
            "Lot bainds onn at\n",
            "Yo:\n",
            "Whit 'ledde, as inglif dor, gah therpay!\n",
            "\n",
            "\n",
            "GARETEN:\n",
            "Lecotieg shot, matin the bundood my waknes Moder with tlether bereennis it hat att.\n",
            "\n",
            "Bay\n",
            "Tuld:\n",
            "I the rat thersters, awill---being wet tohe not trathrogd:\n",
            "I 'hies my an\n",
            "Aspos:\n",
            "SIf If I hot gien mexred't for of bittyeicels meargay that to. APUL:\n",
            "Ttas is bed\n",
            "I dy not thish\n",
            "Is thissetseck.\n",
            "\n",
            "MA:\n",
            "Roumay, ther? I\n",
            "the ot mordner:\n",
            "Thut guhs I dougw Dor whold ay\n",
            "sesto.\n",
            "\n",
            "ISIS'Nilll my\n",
            "Theseed seill dooke.\n",
            "\n",
            "Le\n",
            "And\n",
            "That--\n",
            "way-Yy:\n",
            "In uch arst Math om him pight ewf becothene yecle'f all his for tequenot godewsacy meres,\n",
            "Nould bes, yo it ith sur a lapre sines haunit buste, the moroch, isi.\n",
            "\n",
            "Thy, KI for bitizell, 'avil; to mo, I dinth: thres\n",
            "Hetny then swarlirke he as Rin ot\n",
            "ust? I thy, wiothe of shear gund yilt I asir;\n",
            "Anger the hoval prat, co ssting ther all dukliex ato this,\n",
            "To thold to mys; all be\n",
            "shold's adaiem them? Now, a brine aplo del fend.\n",
            "\n",
            "Trach lone and terived theme\n",
            "An ast a lithe gies ef\n",
            "I foll dingnuch forrm hore tathe beavyen wiliagnn;\n",
            "To, che werest sor not thener?\n",
            "Aave,--row. Hadiaste me anbr ast be forell:\n",
            "With, 'bee heret, will ne abe the pone;\n",
            "N Yokar dowille cagle Dre,---ulin.\n",
            "STL:\n",
            "Combothe und sould tooo so\n",
            "wer the bothe oncarwee: by ay the in Meawn? shall blive hoseee: tohll'vet dime,\n",
            "Sencos: therm Le caim, to hous, ther ke tohd now fornd the un?\n",
            "\n",
            "The ge\n",
            "No ffoll--lloshmee anders,--ame congeat st I I I'll thall; ham cofith astill'lin nuch ot an haim annto the iche me\n",
            "to ath gain chine?\n",
            "\n",
            "Uerres, sed is the buse? I\n",
            "Meld thenn buchstomime.\n",
            "\n",
            "LART:\n",
            "Theacar wall nobe: aginghthen'e\n",
            "tings ghat awill asheact ellh I be onot ish.\n",
            "Ne\n",
            "'shearn, oneke apave be sonn sast?\n",
            "\n",
            "PULar:\n",
            "\n",
            "Re lave\n",
            "Whall of weir Doud aprin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note on multi head attention: adding multi head attention instead of single head attention improved loss from 2.36 to 2.18. Because this module increase the communication between nodes."
      ],
      "metadata": {
        "id": "iYvnwjjJnU7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Add feed forward module to model**"
      ],
      "metadata": {
        "id": "cjJAPD4lnrUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "r05po1QSq1E3"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward_Multi_Haed_Attention(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_head = MultiHeadAttention(4, n_embd//4)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_embd = self.token_embedding_table(idx) # (B, T, C=n_emd)\n",
        "        pos_embd = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_embd + pos_embd  #(B, T, C)\n",
        "        x = self.sa_head(x)\n",
        "        x = self.ffwd(x)\n",
        "        logits = self.lm_head(x)  # (B, T, C=vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    # This function generates all the chunks\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "XizoiD_Iq8zi"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FeedForward_Multi_Haed_Attention()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P69nEJ3wr8cy",
        "outputId": "91417d20-bbf8-4c46-87c3-cc7db8c7c48e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.026881 M parameters\n",
            "step 0: train loss 4.2134, val loss 4.2125\n",
            "step 100: train loss 2.9699, val loss 3.0029\n",
            "step 200: train loss 2.7221, val loss 2.7322\n",
            "step 300: train loss 2.6369, val loss 2.6515\n",
            "step 400: train loss 2.5922, val loss 2.5892\n",
            "step 500: train loss 2.5341, val loss 2.5446\n",
            "step 600: train loss 2.4932, val loss 2.4942\n",
            "step 700: train loss 2.4572, val loss 2.4646\n",
            "step 800: train loss 2.4275, val loss 2.4298\n",
            "step 900: train loss 2.3851, val loss 2.4020\n",
            "step 1000: train loss 2.3569, val loss 2.3733\n",
            "step 1100: train loss 2.3387, val loss 2.3559\n",
            "step 1200: train loss 2.3272, val loss 2.3343\n",
            "step 1300: train loss 2.3038, val loss 2.3175\n",
            "step 1400: train loss 2.2938, val loss 2.3166\n",
            "step 1500: train loss 2.2600, val loss 2.2824\n",
            "step 1600: train loss 2.2591, val loss 2.2761\n",
            "step 1700: train loss 2.2438, val loss 2.2473\n",
            "step 1800: train loss 2.2369, val loss 2.2531\n",
            "step 1900: train loss 2.2116, val loss 2.2355\n",
            "step 2000: train loss 2.1943, val loss 2.2320\n",
            "step 2100: train loss 2.1898, val loss 2.2290\n",
            "step 2200: train loss 2.1836, val loss 2.2100\n",
            "step 2300: train loss 2.1759, val loss 2.2039\n",
            "step 2400: train loss 2.1594, val loss 2.2011\n",
            "step 2500: train loss 2.1576, val loss 2.1892\n",
            "step 2600: train loss 2.1511, val loss 2.1853\n",
            "step 2700: train loss 2.1379, val loss 2.1854\n",
            "step 2800: train loss 2.1316, val loss 2.1622\n",
            "step 2900: train loss 2.1147, val loss 2.1715\n",
            "step 3000: train loss 2.1112, val loss 2.1621\n",
            "step 3100: train loss 2.0990, val loss 2.1604\n",
            "step 3200: train loss 2.1006, val loss 2.1565\n",
            "step 3300: train loss 2.0915, val loss 2.1503\n",
            "step 3400: train loss 2.0901, val loss 2.1449\n",
            "step 3500: train loss 2.0891, val loss 2.1369\n",
            "step 3600: train loss 2.0730, val loss 2.1243\n",
            "step 3700: train loss 2.0710, val loss 2.1294\n",
            "step 3800: train loss 2.0640, val loss 2.1290\n",
            "step 3900: train loss 2.0561, val loss 2.1352\n",
            "step 4000: train loss 2.0549, val loss 2.1177\n",
            "step 4100: train loss 2.0421, val loss 2.1135\n",
            "step 4200: train loss 2.0451, val loss 2.1042\n",
            "step 4300: train loss 2.0394, val loss 2.1128\n",
            "step 4400: train loss 2.0330, val loss 2.1210\n",
            "step 4500: train loss 2.0324, val loss 2.1038\n",
            "step 4600: train loss 2.0354, val loss 2.1213\n",
            "step 4700: train loss 2.0199, val loss 2.1125\n",
            "step 4800: train loss 2.0277, val loss 2.0988\n",
            "step 4900: train loss 2.0115, val loss 2.0901\n",
            "step 4999: train loss 2.0184, val loss 2.1005\n",
            "\n",
            "A ange call dea, teart detu nerouss,\n",
            "The stre, aghne;\n",
            "Mery hill prood, sarivis of thals the hifer mon the hit hat thea do me,\n",
            "UncUS:\n",
            "I your all theig in his asalf. Hathou stellight.\n",
            "I lon borsser Canion herciungen a fishall you of ilase pree.\n",
            "Come elovee is to not a pry son he will ame\n",
            "Grake of-kwarloods here' of vidgeven, by unce, adles,\n",
            "O hights fut hapedmeabut I bells;\n",
            "Uplabud, gap a but the all ay theirts, we a fif he a wers, is you cusenepead pon meove lame,\n",
            "Nake my aporspeact thou of arak. Eralich acand hat a valk on put ofard Lors! a\n",
            "Seffiely him.\n",
            "\n",
            "OLAFLCESSABOLINGHAULENCLRORICHASIO:\n",
            "Care comed and shaill, scald, wre thee,\n",
            "Yere this hat be sarmar, beal; and. will yound somer's a.\n",
            "\n",
            "\n",
            "MERY\n",
            "BUCKINIOLAUS:\n",
            "Ary sprotheat a of 'ter\n",
            "wieess. Eloveder, whell the darands.\n",
            "\n",
            "The low To sord of that bid by by fit:\n",
            "Aures; dir shousell apsercences thang ow,\n",
            "Whee, hous dase rusel' chaks the till cones bay I ever:\n",
            "To bith to hear. All me sart\n",
            "Ralives:\n",
            "I of dounke are, shave allike to ree is keettest for your the she ghath.\n",
            "Was of I malfsick, all he he com!\n",
            "\n",
            "Cow she hor no: boodmpol me:\n",
            "I dow! lad reme, warke of the dearthen, I he mans butercought.\n",
            "\n",
            "Cordy?\n",
            "But uessench, 'akiy o' willof the ment dagacommeren, sher resontlake quirt; thee a host I not, brate\n",
            "Thou of frove nowel, ther a be Clode's allou fit tolights,\n",
            "FRUMIO:\n",
            "Wit telt:\n",
            "Ford wather gineps he a.\n",
            "You mady chan is bitallove; tage a flok, hacon,\n",
            "But, whatle, 'all thout govale\n",
            "Thou was, sham fen?\n",
            "INGSBRES:\n",
            "I'\n",
            "BOSTIF Riond sen thour o'sersciombla\n",
            "PLENTSOT:\n",
            "And lose therefed:\n",
            "It and, oe make do not the gref les thenmegas shil by prigh'd wo feeds fill.\n",
            "\n",
            "Oshall,, shalk forciot wreake and theer,\n",
            "Werey\n",
            "As, I'll Vo teldid nay, I trr, my ald hen ther love the hall brold then his rey the respather,\n",
            "Whe the und laon to spreang thee thys 'told-casward if and sin Mopare well lee prakie;\n",
            "UT to ther'd.\n",
            "\n",
            "DRUKE YORD RIIIII prith whather, hou buter I'ld, and to would wick,\n",
            "But male hough, I he you vend I so hace now\n",
            "To ther:\n",
            "Sills of You a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note on Feedforward module: adding feedforward improved loss from 2.18 to 2.1. Since feedforward module is implemented as a linear layer so it's applying per token level. Attention increase communication between tokens and feedforward make tokens to process the gathered data indivually."
      ],
      "metadata": {
        "id": "0IbZmFBzstTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Create blocks of feedforward and attention modules**"
      ],
      "metadata": {
        "id": "JA0ZHXnUt8lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sa(x)\n",
        "        x = self.ffwd(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "G0I3aU9ouWHH"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block_Multi_Haed_Attention(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_embd = self.token_embedding_table(idx) # (B, T, C=n_emd)\n",
        "        pos_embd = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_embd + pos_embd  #(B, T, C)\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)  # (B, T, C=vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    # This function generates all the chunks\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "4WZx6KW5uuv-"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Block_Multi_Haed_Attention()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAJSQriIvZnd",
        "outputId": "43b5ab3b-92f1-43a8-94c4-facdef871674"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.059777 M parameters\n",
            "step 0: train loss 4.1829, val loss 4.1827\n",
            "step 100: train loss 3.3159, val loss 3.3396\n",
            "step 200: train loss 3.3106, val loss 3.3545\n",
            "step 300: train loss 3.2949, val loss 3.3345\n",
            "step 400: train loss 3.1580, val loss 3.1655\n",
            "step 500: train loss 3.1157, val loss 3.1281\n",
            "step 600: train loss 3.0919, val loss 3.0789\n",
            "step 700: train loss 3.0746, val loss 3.0541\n",
            "step 800: train loss 2.9846, val loss 2.9819\n",
            "step 900: train loss 2.8855, val loss 2.8747\n",
            "step 1000: train loss 2.8069, val loss 2.7947\n",
            "step 1100: train loss 2.7300, val loss 2.7233\n",
            "step 1200: train loss 2.6682, val loss 2.6590\n",
            "step 1300: train loss 2.6322, val loss 2.6248\n",
            "step 1400: train loss 2.5952, val loss 2.5954\n",
            "step 1500: train loss 2.5571, val loss 2.5506\n",
            "step 1600: train loss 2.5525, val loss 2.5511\n",
            "step 1700: train loss 2.5019, val loss 2.4878\n",
            "step 1800: train loss 2.4844, val loss 2.4773\n",
            "step 1900: train loss 2.4638, val loss 2.4537\n",
            "step 2000: train loss 2.4792, val loss 2.4717\n",
            "step 2100: train loss 2.4383, val loss 2.4270\n",
            "step 2200: train loss 2.4103, val loss 2.4009\n",
            "step 2300: train loss 2.4142, val loss 2.4050\n",
            "step 2400: train loss 2.3930, val loss 2.3945\n",
            "step 2500: train loss 2.3791, val loss 2.3830\n",
            "step 2600: train loss 2.3586, val loss 2.3604\n",
            "step 2700: train loss 2.3480, val loss 2.3357\n",
            "step 2800: train loss 2.3442, val loss 2.3377\n",
            "step 2900: train loss 2.3257, val loss 2.3230\n",
            "step 3000: train loss 2.3020, val loss 2.3141\n",
            "step 3100: train loss 2.3047, val loss 2.3132\n",
            "step 3200: train loss 2.2785, val loss 2.2894\n",
            "step 3300: train loss 2.2842, val loss 2.3006\n",
            "step 3400: train loss 2.2670, val loss 2.2869\n",
            "step 3500: train loss 2.2556, val loss 2.2586\n",
            "step 3600: train loss 2.2628, val loss 2.2574\n",
            "step 3700: train loss 2.2303, val loss 2.2410\n",
            "step 3800: train loss 2.2367, val loss 2.2518\n",
            "step 3900: train loss 2.2125, val loss 2.2346\n",
            "step 4000: train loss 2.2166, val loss 2.2347\n",
            "step 4100: train loss 2.2035, val loss 2.2235\n",
            "step 4200: train loss 2.1960, val loss 2.2231\n",
            "step 4300: train loss 2.1996, val loss 2.2259\n",
            "step 4400: train loss 2.1760, val loss 2.1988\n",
            "step 4500: train loss 2.1729, val loss 2.1901\n",
            "step 4600: train loss 2.1619, val loss 2.1923\n",
            "step 4700: train loss 2.1573, val loss 2.1761\n",
            "step 4800: train loss 2.1412, val loss 2.1712\n",
            "step 4900: train loss 2.1459, val loss 2.1743\n",
            "step 4999: train loss 2.1401, val loss 2.1658\n",
            "\n",
            "KIZGTONELE:\n",
            "Hary and that de at basbe sote momee\n",
            "That mane, meree woock sle grolegey: dow not mam; cow at ceanen,\n",
            "Whoe.\n",
            "\n",
            "Serlte:T you shal me Gube, lay't is I seal,\n",
            "Coroace itstrlok: ind!\n",
            "Th, hate pare im you be faver midireres til I I neas whowas yourn is of are fancie wing at.\n",
            "Grur; to w hal me? ich low woughens has bre.\n",
            "\n",
            "In louvereres non,\n",
            "Has?\n",
            "\n",
            "LENH:\n",
            "Ay I nawtheny sour dlineden?\n",
            "\n",
            "QUENLANCUCY:\n",
            "Ne Id lible sieged cacer, sunt it henne urior,\n",
            "Herefes beanow, how aninglef sou.\n",
            "\n",
            "GLECHESOUS:\n",
            "Bistamem me fine grough finethre:\n",
            "Stay dos, somony nestour me: agh o,\n",
            "I byar hew that the ke yese mined the!\n",
            "Gamer, I sanothe ofsm? Lim the'd dull copebes die visers,\n",
            "'Cich, his shou mo twith le bave Meckiarbines,\n",
            "And pamy love.\n",
            "\n",
            "SARINPSHAM LARDUS:\n",
            "Bilpe my iut lou bef's if thenove\n",
            "Pringingm nizinee, brent'd, Is you my, If be: frest to me athist:\n",
            "I was win roorst; my ant meshe gin sither it I swhas wome leef lighy say tinge,\n",
            "Feif, taled alle to pas soith Tome, amoves tace we wighemys my, coee.\n",
            "\n",
            "KUCHUTIOUS:\n",
            "Hind hat rrenet, Frokonthmo't he no,\n",
            "Their asacherer; ber ther ouis, wom Qing me\n",
            "The powallok lon\n",
            "venn'sw fode marosely ans be thalls.\n",
            "\n",
            "Howmeen laitt and ilsth,\n",
            "O wo maw in thim arine.\n",
            "By, Ifrorel\n",
            "Ad fonoth: banters mieers opame,\n",
            "Sele may my derfmenedts nerers; raind?\n",
            "That not wieth his Fid of, bun was kine im seme andol,--'ten,\n",
            "Me ot ame theren of ashater foome, hemar voonche hate shin me thou ame,\n",
            "Werca onn thysance.' hene-sarse!\n",
            "I san, gool qese Lorost peapte alenset weir\n",
            "Tand therp, arthises, frualasay drepwys opst thun?\n",
            "\n",
            "Lele'r shoy,\n",
            "Bpieghe vave whis save hen Le's'd foughe, nawhy bavigens soued fe owher your fevere\n",
            "And pigherrur of tithis ame you Rroe'st my sos isam mack,\n",
            "Wake then, Mathit o'\n",
            "Fess I do lart nothe hime, leads the oures lied. him ve mom are ut o' gourtt.\n",
            "\n",
            "Glored wave\n",
            "I dpanes fringt criomou: she gu.\n",
            "\n",
            "Gerear sie;\n",
            "That dilkte, be thas wepkens of thy'st talle.\n",
            "Gakfewh shate, Leadebitel, as biltes,\n",
            "Geathy wish,\n",
            "Yonnorsf fink she erighem, is fit-alantioous'd:\n",
            "You,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding blocks didnt improve the loss because these blocks make the model deeper and it makes optimizations issues. for solve this problem we add skip connection and layer normalization."
      ],
      "metadata": {
        "id": "N1VTZjt0v2X4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Add skip connections and layer normalization**"
      ],
      "metadata": {
        "id": "l3c-qLTpw1Pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d:\n",
        "  \n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]"
      ],
      "metadata": {
        "id": "A2-ZAIbyyW3Q"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        # self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        # out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "mRQPPBlvxSER"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4*n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*n_embd, n_embd)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "jaDPQBCcxmYA"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "NdYH18udw8xT"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Modified_Multi_Haed_Attention(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_embd = self.token_embedding_table(idx) # (B, T, C=n_emd)\n",
        "        pos_embd = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_embd + pos_embd  #(B, T, C)\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)  # (B, T, C=vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    # This function generates all the chunks\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "MQxR-rX_xQ4z"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Modified_Multi_Haed_Attention()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhNaNqYlzxIE",
        "outputId": "27a61c99-b6fc-4170-a144-24e3ba11b598"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.159809 M parameters\n",
            "step 0: train loss 4.5011, val loss 4.5139\n",
            "step 100: train loss 2.6538, val loss 2.6712\n",
            "step 200: train loss 2.5167, val loss 2.5290\n",
            "step 300: train loss 2.4580, val loss 2.4607\n",
            "step 400: train loss 2.3907, val loss 2.4106\n",
            "step 500: train loss 2.3428, val loss 2.3575\n",
            "step 600: train loss 2.2865, val loss 2.3020\n",
            "step 700: train loss 2.2504, val loss 2.2626\n",
            "step 800: train loss 2.2095, val loss 2.2283\n",
            "step 900: train loss 2.1741, val loss 2.1992\n",
            "step 1000: train loss 2.1350, val loss 2.1814\n",
            "step 1100: train loss 2.0983, val loss 2.1413\n",
            "step 1200: train loss 2.0800, val loss 2.1111\n",
            "step 1300: train loss 2.0458, val loss 2.1031\n",
            "step 1400: train loss 2.0253, val loss 2.0856\n",
            "step 1500: train loss 2.0048, val loss 2.0764\n",
            "step 1600: train loss 1.9964, val loss 2.0577\n",
            "step 1700: train loss 1.9667, val loss 2.0484\n",
            "step 1800: train loss 1.9524, val loss 2.0370\n",
            "step 1900: train loss 1.9412, val loss 2.0154\n",
            "step 2000: train loss 1.9216, val loss 2.0190\n",
            "step 2100: train loss 1.9043, val loss 2.0081\n",
            "step 2200: train loss 1.9005, val loss 2.0014\n",
            "step 2300: train loss 1.8801, val loss 1.9835\n",
            "step 2400: train loss 1.8662, val loss 1.9811\n",
            "step 2500: train loss 1.8668, val loss 1.9873\n",
            "step 2600: train loss 1.8535, val loss 1.9632\n",
            "step 2700: train loss 1.8410, val loss 1.9524\n",
            "step 2800: train loss 1.8284, val loss 1.9482\n",
            "step 2900: train loss 1.8092, val loss 1.9525\n",
            "step 3000: train loss 1.8261, val loss 1.9426\n",
            "step 3100: train loss 1.8024, val loss 1.9335\n",
            "step 3200: train loss 1.7767, val loss 1.9185\n",
            "step 3300: train loss 1.7805, val loss 1.9219\n",
            "step 3400: train loss 1.7799, val loss 1.9215\n",
            "step 3500: train loss 1.7710, val loss 1.8978\n",
            "step 3600: train loss 1.7583, val loss 1.8945\n",
            "step 3700: train loss 1.7510, val loss 1.8918\n",
            "step 3800: train loss 1.7471, val loss 1.9066\n",
            "step 3900: train loss 1.7455, val loss 1.8806\n",
            "step 4000: train loss 1.7458, val loss 1.8947\n",
            "step 4100: train loss 1.7284, val loss 1.8838\n",
            "step 4200: train loss 1.7233, val loss 1.8826\n",
            "step 4300: train loss 1.7293, val loss 1.8731\n",
            "step 4400: train loss 1.7215, val loss 1.8552\n",
            "step 4500: train loss 1.7221, val loss 1.8739\n",
            "step 4600: train loss 1.7157, val loss 1.8577\n",
            "step 4700: train loss 1.7145, val loss 1.8624\n",
            "step 4800: train loss 1.7122, val loss 1.8672\n",
            "step 4900: train loss 1.6946, val loss 1.8702\n",
            "step 4999: train loss 1.6925, val loss 1.8427\n",
            "\n",
            "To bate ea advords, with him dear thouse: 'Fal day me mush serves good both in say;\n",
            "For I, do my the putch donce is are best,\n",
            "For stript thas mards; what abowixed\n",
            "this appy'es pace defather bout preasor, I would ve togar.\n",
            "\n",
            "SING RICHARN II:\n",
            "Priny and it:\n",
            "That! We form I ne Forpant?\n",
            "\n",
            "USHORY:\n",
            "Beme-to that O prodiour Son: both.\n",
            "\n",
            "Fie, Bidlukes, we as quarraws and yound wilcomes oure,\n",
            "No, beld fir, foot I trace such uper\n",
            "So my my fail to and the dephis deast the facemy hendely aber spowe,\n",
            "The leve so happenth a good to what night march lord;\n",
            "For that defuck you, it day'l; do look,\n",
            "Ay, I was dief\n",
            "both, sir, with he paraint, I peay is beat; sweeth that exquarry! I wast come her, noot onceese vourses for\n",
            "kithews it stubst and he: nows no are I.\n",
            "3ho I weseep him hath a seat: '\n",
            "Comson longe of the worth that so over.\n",
            "\n",
            "BANTILINUS:\n",
            "You hast why adnes,\n",
            "To fut a know fortim so, amme: me'l.\n",
            "\n",
            "ROMard's with you change\n",
            "Farster senieve stay well of the know\n",
            "The fundious lardy!' whith\n",
            "ONs: that we hast his be looks to the Edwardle:' sir.\n",
            "\n",
            "COMINIUS:\n",
            "Why, my lordon\n",
            "Your Allandy, rest had find? the bast with of the ne'er word\n",
            "That my doem owt were my some the the deviser; Edrook.\n",
            "\n",
            "ANGS:\n",
            "\n",
            "DUKE OF AUNT:\n",
            "Whaty, sence discesumn for have teant,\n",
            "Thy day beford's off thee consemfame,\n",
            "What stay by so the prass'sutings by dance unme dreven'd begging\n",
            "To Murcier:\n",
            "As me! the be'st to me to mystle and word.\n",
            "Comily exlly shall\n",
            "Cousion they uncause, that looks,\n",
            "Tonce as no suys in ressun\n",
            "the crareaf' sweet a'eway; but the have stitness\n",
            "Of to ruch easue stisen's feed, to\n",
            "say both marce\n",
            "With of trought givines\n",
            "To annot-their? Of with look Pary o' time!\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "So\n",
            "Desest greadlens sconfest-iterself-dear to wh wh chow not\n",
            "Or and it en? 'He's he thy more to thre anman,\n",
            "As lind, and Gent sea of reath unpost most the which.\n",
            "\n",
            "SOMNIUS:\n",
            "My dranward,\n",
            "That I santing ear harmsed.\n",
            "\n",
            "COMINIUS:\n",
            "\n",
            "HENRY PEt Kil VORCK:\n",
            "Be maife thy Dimmitience;\n",
            "I would see his hate the lace of;\n",
            "Jest it are thou, sir, wify that go c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding skip connections and layer normalization improved loss from 2.1 to 1.84 but model has been overfitted on train data so we can add dropout to solve this problem."
      ],
      "metadata": {
        "id": "fI8txNHJ020d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Add dropout**"
      ],
      "metadata": {
        "id": "S7h5Ux5k1NXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "mtAJIgHp2vf6"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "uiph1F-D1P24"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "ro938Xor1mSW"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "3RZ9FaV11mOy"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel_Final_Version(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "fpnPRMU51mMn"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel_Final_Version()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNt38gxX1l_E",
        "outputId": "7238e7c2-078d-4595-cd15-2031abf711de"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n",
            "step 0: train loss 4.3200, val loss 4.3140\n",
            "step 500: train loss 1.9962, val loss 2.0883\n",
            "step 1000: train loss 1.6020, val loss 1.7877\n",
            "step 1500: train loss 1.4351, val loss 1.6545\n",
            "step 2000: train loss 1.3344, val loss 1.5711\n",
            "step 2500: train loss 1.2687, val loss 1.5427\n",
            "step 3000: train loss 1.2154, val loss 1.5174\n",
            "step 3500: train loss 1.1719, val loss 1.5125\n",
            "step 4000: train loss 1.1294, val loss 1.5122\n",
            "step 4500: train loss 1.0887, val loss 1.5239\n",
            "step 4999: train loss 1.0506, val loss 1.5375\n",
            "\n",
            "CAPULET:\n",
            "A husband! how these instruments foul gentle out:\n",
            "Let how me strike him on me 'gainst the puls;\n",
            "I will away; I never writ.\n",
            "\n",
            "Second Servant:\n",
            "Farewell: it well.\n",
            "\n",
            "Shird:\n",
            "Gi, he is it me wonder'd; and all sure was fretch to well.\n",
            "\n",
            "Second Servant:\n",
            "\n",
            "Provost:\n",
            "What, we have resolved; and we are baseful our thanks.\n",
            "\n",
            "Second Servingman:\n",
            "By heavy, sweet our ancient catch was the fire\n",
            "Is of your schold blood, God sake wings; timew-to happy\n",
            "on that crubblech'd moan, ffoes, speeding over ships,\n",
            "but thou, what touch'st that: cover'd plouch'd, the\n",
            "dum wounds, the Lord of Hereford professored! O, my way!\n",
            "And the is the vextage of comfort. Isabelly\n",
            "Who shall have alved, his daughter land? My lord, my lord.\n",
            "\n",
            "ANGELO:\n",
            "D take more word: yet truly shirst.\n",
            "\n",
            "LUCIO:\n",
            "But folk, you high.\n",
            "\n",
            "Is ROS:\n",
            "A sic, the tardy call; I say, you kneel with pursuit, percious, waked\n",
            "tybalt, when, Tribus, whom, should undurate your queen?\n",
            "\n",
            "EXETER:\n",
            "Nurse; we are right in France, to his open exleasion.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "If be a little, boy, and all that would swear\n",
            "Be not received us sleeps.\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "HASTINPH:\n",
            "Wilt thee distingned there?\n",
            "\n",
            "GLOUCESTER:\n",
            "I wonderly, Clifford thy cease,\n",
            "Those boy, it like so more withal durage sorrow'd;\n",
            "Yonder Mash'd attend thy answer haste:\n",
            "I rushly thousand wilt upon the house,\n",
            "And follow in alteration of drink\n",
            "The sepulvest with a fulleration\n",
            "Awhile out with at that God's sweet venom'd;\n",
            "And what wat with us teat us and used took waste,\n",
            "Which were they an a soldiers for a packe.\n",
            "What hast thou swoon'dst a woman revenge,\n",
            "Antive the love we fear'd it, and in her here,\n",
            "To blood full as thou errats a shadow-sett\n",
            "As to ten the fire one dully-a place\n",
            "To see my guest-destry running design,--fellowy todds--\n",
            "Well-him I would must, with flowers westand know\n",
            "Frayed that corner.--How now?\n",
            "\n",
            "VOLUMNIA:\n",
            "Well, sir, I mean:\n",
            "I will speak for this more woman.\n",
            "\n",
            "MERCUTIO:\n",
            "Think what is a worthy Prisal kell'd him,\n",
            "I would make sweet bold his foreign to be so,\n",
            "And in my royalt vantage and with h\n"
          ]
        }
      ]
    }
  ]
}